from generate_imdp import learn_imdp, normalise_imdp
from gridworld import MDP
from gridworld_utils import convert_transition_matrix_to_julia_imdp
from imdp_cf_bounds import MultiStepCFBoundCalculator as IMDPMultiStepCFBoundCalculator
from mdp_cf_bounds import MultiStepCFBoundCalculator as MDPMultiStepCFBoundCalculator
import numpy as np
import sys
from utils import save_imdp_to_file, format_transition_matrix_for_julia, load_generated_policy


def sample_probabilities_within_intervals(lower, upper):    
    while True:
        # Sample uniformly within the given bounds.
        sampled_probs = np.random.uniform(lower, upper)
        
        # Normalise the sampled probabilities to sum to 1.
        sampled_probs /= np.sum(sampled_probs)
        sampled_probs /= np.sum(sampled_probs)

        # Allowed error due to floating-point errors.
        epsilon = 1e-13

        # Check if the normalized probabilities still lie within the bounds.
        if np.all(sampled_probs >= lower-epsilon) and np.all(sampled_probs <= upper+epsilon):
            return sampled_probs


def sample_CFMDP(interval_CF_MDP, n_timesteps=10, n_states=16, n_actions=4):
    CFMDP = np.zeros(shape=(n_timesteps, n_states, n_actions, n_states))

    for t in range(n_timesteps):
        for s in range(n_states):
            for a in range(n_actions):
                CFMDP[t, s, a] = sample_probabilities_within_intervals(interval_CF_MDP[t, s, a, :, 0], interval_CF_MDP[t, s, a, :, 1])

    return CFMDP


def evaluate_policies(MDP_rewards, true_pi, simulated_pi, interval_CF_MDP, n_steps = 10):
    n_state = 4
    N_CFMDPS = 10
    N_TRAJECTORIES = 10000

    all_rewards_simulated_icfmdp = []
    all_rewards_icfmdp = []
    
    for k in range(N_CFMDPS):
        print(f"{k}/{N_CFMDPS}")
        CFMDP = sample_CFMDP(interval_CF_MDP)

        # Test with true ICFMDP policy.
        for j in range(N_TRAJECTORIES):
            print(f"{j}/{N_TRAJECTORIES}")
            trajectory = np.zeros((n_steps, n_state))
            current_state = 0

            for time_idx in range(n_steps):
                action = true_pi[time_idx, current_state]
                next_state = np.random.choice(16, size=1, p=CFMDP[time_idx, current_state, action])[0] 
                
                reward = MDP_rewards[current_state, action]
                trajectory[time_idx, :] = np.array([current_state, next_state, action, reward])
                current_state = next_state

            rewards = trajectory[:, 3]

            all_rewards_icfmdp.append(rewards)

        # Test with policy learned over simulated IMDP.
        for j in range(N_TRAJECTORIES):
            print(f"{j}/{N_TRAJECTORIES}")

            trajectory = np.zeros((n_steps, n_state))
            current_state = 0

            for time_idx in range(n_steps):
                action = simulated_pi[time_idx, current_state]
                next_state = np.random.choice(16, size=1, p=CFMDP[time_idx, current_state, action])[0] 
                reward = MDP_rewards[current_state, action]
                trajectory[time_idx, :] = np.array([current_state, next_state, action, reward])
                current_state = next_state

            rewards = trajectory[:, 3]
            all_rewards_simulated_icfmdp.append(rewards)

    return all_rewards_icfmdp, all_rewards_simulated_icfmdp


def main():
    if len(sys.argv) < 2:
        print("Usage: python gridworld.py <function_name>")
        sys.exit(1)
    
    function_name = sys.argv[1]

    mdp = MDP()

    if function_name == "train":
        # Generate IMDP from simulations.
        simulated_imdp = learn_imdp(mdp)

        # Generate observed path, and normalise IMDP.
        observed_path = mdp.sample_suboptimal_trajectory()
        simulated_imdp = normalise_imdp(simulated_imdp, observed_path)
        save_imdp_to_file("gridworld_imdp.txt", simulated_imdp)

        # Generate CFIMDP from simulated IMDP.
        cf_bound_calculator = IMDPMultiStepCFBoundCalculator(simulated_imdp)
        simulated_cf_imdp = cf_bound_calculator.calculate_bounds(observed_path)

        simulated_julia_cf_imdp = format_transition_matrix_for_julia(simulated_cf_imdp, 10, 16, 4)
        convert_transition_matrix_to_julia_imdp(simulated_julia_cf_imdp, filename="MDPs/simulated_gridworld.jl")

        # Generate CFIMDP from true IMDP.
        cf_bound_calculator = MDPMultiStepCFBoundCalculator(mdp.transition_matrix)
        true_cf_imdp = cf_bound_calculator.calculate_bounds(observed_path)

        true_julia_cf_imdp = format_transition_matrix_for_julia(true_cf_imdp, 10, 16, 4)
        convert_transition_matrix_to_julia_imdp(true_julia_cf_imdp, filename="MDPs/gridworld.jl")

    elif function_name == "test":
        # TODO: compare CFIMDPs in terms of bound widths.
        # TODO: compare value functions and policies generated by both policies.
        # TODO: need to compare with different hyper-parameters that we use when simulating the IMDP

        observed_path = mdp.sample_suboptimal_trajectory()
        true_pi = load_generated_policy(f"ICFMDPs/true_gridworld_policy.jld2", 10, 16)
        simulated_pi = load_generated_policy(f"ICFMDPs/true_gridworld_policy.jld2", 10, 16)

        # True CFIMDP
        cf_bound_calculator = MDPMultiStepCFBoundCalculator(mdp.transition_matrix)
        true_cf_imdp = cf_bound_calculator.calculate_bounds(observed_path)

        all_icfmdp_rewards, all_simulated_rewards = evaluate_policies(mdp.rewards, true_pi, simulated_pi, true_cf_imdp)

        all_icfmdp_rewards = np.array(all_icfmdp_rewards).reshape(100000, 10)
        all_simulated_rewards = np.array(all_simulated_rewards).reshape(100000, 10)

        mean_icfmdp_rewards = np.mean(np.array(all_icfmdp_rewards), axis=0)
        std_icfmdp_rewards = np.std(np.array(all_icfmdp_rewards), axis=0)
        mean_simulated_rewards = np.mean(np.array(all_simulated_rewards), axis=0)
        std_simulated_rewards = np.std(np.array(all_simulated_rewards), axis=0)

        upper_icfmdp_errors = np.clip(mean_icfmdp_rewards + std_icfmdp_rewards, None, 100) - mean_icfmdp_rewards
        upper_simulated_errors = np.clip(mean_simulated_rewards + std_simulated_rewards, None, 100) - mean_simulated_rewards
        lower_icfmdp_errors = mean_icfmdp_rewards - np.clip(mean_icfmdp_rewards - std_icfmdp_rewards, -100, None)
        lower_simulated_errors = mean_simulated_rewards - np.clip(mean_simulated_rewards - std_simulated_rewards, -100, None)

        with open(f"GridWorld Results.txt", "a") as file:
            file.write(f"Observed trajectory: {observed_path}\n\n")
            file.write(f"Average Results: \n\n")
            file.write(f"Mean ICFMDP rewards: {mean_icfmdp_rewards}\n")
            file.write(f"Upper bounds: {upper_icfmdp_errors}\n")
            file.write(f"Lower bounds: {lower_icfmdp_errors}\n")
            file.write(f"Mean simulated ICFMDP rewards: {mean_simulated_rewards}\n")
            file.write(f"Upper bounds: {upper_simulated_errors}\n")
            file.write(f"Lower bounds: {lower_simulated_errors}\n\n")

    else:
       print(f"Function '{function_name}' is not recognized.")

main()
